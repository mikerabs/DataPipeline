# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  spark_stream.py   â€“   Kafka â†’ Cassandra streaming
#                       topic : users_created
#                       table : spark_demo.users_created
#                       PK    : email  (text, NOT NULL)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StringType

# â”€â”€ 1. Spark session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spark = (
    SparkSession.builder
        .appName("Kafka â†’ Cassandra : users_created")
        .config(
            "spark.jars.packages",
            ",".join([
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1",
                "com.datastax.spark:spark-cassandra-connector_2.12:3.4.1",
                "com.github.jnr:jnr-posix:3.1.16"
            ])
        )
        .config("spark.cassandra.connection.host", "cassandra")
        .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# â”€â”€ 2. Schema that exactly matches the JSON payload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kafka_schema = (
    StructType()
        .add("first_name", StringType())
        .add("last_name",  StringType())
        .add("email",      StringType(), nullable=False)   # PK â‡’ NOT NULL
)

# â”€â”€ 3. Read & parse Kafka JSON messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kafka_df = (
    spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "broker:29092")
        .option("subscribe", "users_created")
        # Use "earliest" while youâ€™re reâ€‘testing; switch to "latest" in prod
        .option("startingOffsets", "earliest")
        .load()
        .selectExpr("CAST(value AS STRING) AS json")
)

parsed_df = (
    kafka_df
        .select(from_json(col("json"), kafka_schema).alias("data"))
        .select("data.*")
        .filter(col("email").isNotNull())        # safety for the PK
)

# â”€â”€ 4. Write each microâ€‘batch to Cassandra â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_to_cassandra(batch_df, _epoch_id):
    (
        batch_df.write
            .format("org.apache.spark.sql.cassandra")
            .mode("append")
            .options(keyspace="spark_demo", table="users_created")
            .save()
    )

query = (
    parsed_df.writeStream
        .foreachBatch(write_to_cassandra)
        .option("checkpointLocation", "/opt/bitnami/spark/checkpoints/users_created")
        .start()
)

print("ðŸš€  Streaming query RUNNING â€” waiting for messages on topic â€˜users_createdâ€™ â€¦")
query.awaitTermination()
